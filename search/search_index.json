{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DFIR-IRIS Blog {{ blog_content }}","title":"Home"},{"location":"#dfir-iris-blog","text":"{{ blog_content }}","title":"DFIR-IRIS Blog"},{"location":"deep_dives/","text":"Deep Dives {{ blog_content deep_dives }}","title":"Deep Dives"},{"location":"deep_dives/#deep-dives","text":"{{ blog_content deep_dives }}","title":"Deep Dives"},{"location":"tags/","text":"{{ tag_content }}","title":"Tags"},{"location":"blog/iris_mid_year_summary/","tags":["Story"],"text":"A mid-year review The idea of DFIR-IRIS is born within the commercial CSIRT of Airbus Cybersecurity in France, 2019. Following the struggle to share technical details during engagements, and after testing multiple existing tools, we figured a custom solution might be needed to fit the team's needs. Consequently the very first draft of DFIR-IRIS was born, developed by two of the team members. Long story short, the tool slowly evolved and improved over time, to the rhythm of the incidents. From a single user mode single page application, it slowly became the multi users multi purpose web application better known today. Comes ends of 2020 and one of the two core members leaving the company, along with the very first mention of releasing DFIR-IRIS in open-source. A year later and some long coding hours behind the scene, the project is finally released in December 2021 with the agreement and support of Airbus CyberSecurity. While the project was far from perfect, we privileged the Release Early, Release Often angle to quickly drive it to a more mature and stable state. From then, we saw a growing interest in the project, a great evolution for the short time it's been published and a definitive place for it in the open source community. As of this article, more than 800 commits were added, 21k lines of code wrote, 14k deleted, 63 issues raised, 48 pulls requests submitted and 8 versions released in less than six months. We added new features along the way such as timeline visualizations, sharing links, hooks, processing modules, VT and MISP integrations in beta, you name it. We also greatly improved the stability and reduce bugs while making the platform quicker and more efficient. And we'd definitely like to thank the community for helping out in achieving this. Looking forward, we are currently assessing the best options to keep the project going in the right direction and prevent any unfortunate slippage. Motivated by how things went so far, we wish to make the project thrive and evolve on the long run with the help of the community. We have buckets of ideas, and we can't wait to share them. Happy DFIR @whitekernel & @ekto #Story .md-typeset .blogging-tags-grid { display: flex; flex-direction: row; flex-wrap: wrap; gap: 8px; margin-top: 5px; } .md-typeset .blogging-tag { color: var(--md-typeset-color); background-color: var(--md-typeset-code-color); } .md-typeset .blogging-tag code { border-radius: 5px; }","title":"A mid-year review"},{"location":"blog/iris_mid_year_summary/#a-mid-year-review","text":"The idea of DFIR-IRIS is born within the commercial CSIRT of Airbus Cybersecurity in France, 2019. Following the struggle to share technical details during engagements, and after testing multiple existing tools, we figured a custom solution might be needed to fit the team's needs. Consequently the very first draft of DFIR-IRIS was born, developed by two of the team members. Long story short, the tool slowly evolved and improved over time, to the rhythm of the incidents. From a single user mode single page application, it slowly became the multi users multi purpose web application better known today. Comes ends of 2020 and one of the two core members leaving the company, along with the very first mention of releasing DFIR-IRIS in open-source. A year later and some long coding hours behind the scene, the project is finally released in December 2021 with the agreement and support of Airbus CyberSecurity. While the project was far from perfect, we privileged the Release Early, Release Often angle to quickly drive it to a more mature and stable state. From then, we saw a growing interest in the project, a great evolution for the short time it's been published and a definitive place for it in the open source community. As of this article, more than 800 commits were added, 21k lines of code wrote, 14k deleted, 63 issues raised, 48 pulls requests submitted and 8 versions released in less than six months. We added new features along the way such as timeline visualizations, sharing links, hooks, processing modules, VT and MISP integrations in beta, you name it. We also greatly improved the stability and reduce bugs while making the platform quicker and more efficient. And we'd definitely like to thank the community for helping out in achieving this. Looking forward, we are currently assessing the best options to keep the project going in the right direction and prevent any unfortunate slippage. Motivated by how things went so far, we wish to make the project thrive and evolve on the long run with the help of the community. We have buckets of ideas, and we can't wait to share them. Happy DFIR @whitekernel & @ekto #Story .md-typeset .blogging-tags-grid { display: flex; flex-direction: row; flex-wrap: wrap; gap: 8px; margin-top: 5px; } .md-typeset .blogging-tag { color: var(--md-typeset-color); background-color: var(--md-typeset-code-color); } .md-typeset .blogging-tag code { border-radius: 5px; }","title":"A mid-year review"},{"location":"deep_dives/custom_attributes_dive/","tags":["Tips","Custom Attributes"],"text":"A deep dive into Custom Attributes In IRIS v1.4.0 we introduced the concept of Custom Attributes, a way to extend the default fields of any case objects. We already published some documentation about it, but today we are exploring the full potential of custom attributes. A basic example Let's start with a simple example and extend the Evidences objects. We will : Add a new checkbox to allow analysts to indicate whether they analyzed the evidence or not, Add a text field to allow them write some notes about the analysis Heading to the attributes administration page in Advanced > Custom Attributes , we open the custom attributes for Evidences. The window should looks like this unless you already added some custom attributes. The right text input allows us to describe the custom attributes thanks to a simple JSON formatting. They are defined as follow : 1 2 3 4 5 6 7 8 9 { \"New Tab Name\" : { \"New Field Name\" : { \"type\" : \"Field type\" , \"mandatory\" : false , \"value\" : \"default value\" } } } Line 2 creates a new tab in the object Line 3 creates a new field within this new tab Line 4 to 6 describes the type and default values of this new field. Field types are predefined types, described in attributes taxonomy . So let's translate this into our example. We want to create a new tab called \"Analysis\" in Evidences object, and within these two new fields : A checkbox \"Has been analyzed\" A text field \"Analysis notes\" Checkbox is of type input_checkbox , and the text field is of type input_textfield . Putting it all together : Basic attribute 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"Analysis\" : { \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } By setting mandatory to false, we ensure the creation/update of Evidences are not rejected if these fields are not set by the users. value must be set to a boolean for checkbox inputs. It can be set to nothing for the text field. Now it's time to ensure our attributes are rendered as expected. Let's click on preview , and then on the - hopefully - new tab Analysis on the preview. There we go ! We can now deploy our changes to the Evidence objects. Curious about how it's handled under the hood ? You can take a look a the deep dive section Under the hood . We will deploy the changes to all Evidences objects but not force the overwrite if some of them already have attributes. We close the preview and we click on Update . Depending on how many Evidences you have, this can take a little time, but once the update is finished, all Evidences objects should have our new custom attributes. We head to the Evidence section of one of our case, click on one of them or try to add one, and we should have a beautiful new tab in our Evidence. Input a few notes, check the box and click on Update . The data is now saved with the object. Time to celebrate ! Dynamic custom attributes That's great, but now what if we want to add the possibility to specify who analyzed the Evidence ? We could add a new text input, but let's make something a little more advanced by proposing a list of all the analysts on the platform. Raw HTML fields There is no direct way of proposing a list of analysts with standard custom attributes, however we have the powerful html type and thanks to that we can input HTML and so Javascript. HTML custom attributes are not processed by IRIS and are rendered as-is in the objects. Let's take the previous Evidence custom attribute and add a new html entry to it, with a simple h3 entity as HTML content. Dynamic attribute 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"Analysis\" : { \"Analyzed by JS\" : { \"type\" : \"html\" , \"mandatory\" : false , \"value\" : \"<h3>Hello IRIS</h3>\" }, \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } Which gives Nice, we can now add raw HTML to our custom attribute. IRIS uses Bootstrap to ease the UI/UX interface, so we can directly reuse these components. Let's replace the h3 with a Bootstrap Select. Note Unfortunately JSON does not support multiline strings, so to ease the reading we provide a prettified version of the HTML separately, and next to it, the corresponding final Custom Attribute definition. HTML template value field Corresponding custom attribute definition 1 2 < label > Analyzed by </ label > < select class = 'selectpicker form-control' ></ select > 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"Analysis\" : { \"Analyzed by JS\" : { \"type\" : \"html\" , \"mandatory\" : false , \"value\" : \"<label>Analyzed by</label><select class='selectpicker form-control'></select>\" }, \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } If you preview this, you'll notice that you see nothing. That's because we need to add some JS to make the select load. First we give the select an ID ( evidence_analyst_analysis ) so we can reference it in the JS, and then call the SelectPicker JS loader on it. HTML template value field Corresponding custom attribute definition 1 2 3 4 5 6 7 8 9 10 11 12 < label > Analyzed by </ label > < select class = 'selectpicker form-control' id = 'evidence_analyst_analysis' ></ select > < script > $ ( '#evidence_analyst_analysis' ). selectpicker ({ liveSearch : true , title : 'Analyst' , style : \"btn-outline-white\" }); </ script > 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"Analysis\" : { \"Analyzed by JS\" : { \"type\" : \"html\" , \"mandatory\" : false , \"value\" : \"<label>Analyzed by</label><select class='selectpicker form-control' id='evidence_analyst_analysis'></select><script>$('#evidence_analyst_analysis').selectpicker({liveSearch: true,title: 'Analyst',style: 'btn-outline-white'});</script>\" }, \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } We can now preview it. Good, we're on the right way ! Requesting external resources We now need to fill the select picker with the available analysts names. Fortunately, we have an API endpoint for that and we can add some JS to request it and fill our select . There's a few JS glue already written within IRIS, so we're going to use that to ease the writing. HTML template value field Corresponding custom attribute definition 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 < label > Analyzed by </ label > < select class = 'selectpicker form-control' id = 'evidence_analyst_analysis' ></ select > < script > $ ( '#evidence_analyst_analysis' ). selectpicker ({ liveSearch : true , title : 'Analyst' , style : 'btn-outline-white' }); get_request_api ( '/manage/users/restricted/list' ) . done (( response ) => { for ( index in response . data ) { $ ( '#evidence_analyst_analysis' ). append ( `<option value=' ${ response . data [ index ]. user_id } '> ${ response . data [ index ]. user_name } </option>` ); } $ ( '#evidence_analyst_analysis' ). selectpicker ( 'refresh' ); }); </ script > 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"Analysis\" : { \"Analyzed by JS\" : { \"type\" : \"html\" , \"mandatory\" : false , \"value\" : \"<label>Analyzed by</label><select class='selectpicker form-control' id='evidence_analyst_analysis'></select><script>$('#evidence_analyst_analysis').selectpicker({liveSearch: true,title: 'Analyst',style: 'btn-outline-white'});get_request_api('/manage/users/restricted/list').done((response) => {for (index in response.data) {$('#evidence_analyst_analysis').append(`<option value='${response.data[index].user_id}'>${response.data[index].user_name}</option>`);}$('#evidence_analyst_analysis').selectpicker('refresh');});</script>\" }, \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } Let's break it down a little. Line 2 declares the select picker for our analysts to choose a name from Lines 4 to 8 initiate the selectpicker lib on the select, so we have live search, bootstrap theme etc. Line 10 asynchronously requests the API endpoint to get a list of the users Lines 11 to 15 is a promise that is called upon request completion, add the users options to the select and finally refresh the select with the newly added options. If we now look at this new code, we should see the list of analysts. We are getting close ! Now, if you deploy these changes and try to save the information of our tab, you'll see that it doesn't work and only the Analysis note and Has been analyzed fields are saved. The Analyst information is lost. That's because HTML custom attributes cannot be saved by users, otherwise this would open the platform to a multitude of vulnerabilities. Please see section Values saving for more information. What we can do instead, is add a new input field and use it as a saving mechanism. Saving values with HTML fields So let's add an input_string field named User ID in our custom attribute, just below the HTML one. This should look like this. Addition of an input string 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"Analysis\" : { \"Analyzed by JS\" : { \"type\" : \"html\" , \"mandatory\" : false , \"value\" : \"<label>Analyzed by</label><select class='selectpicker form-control' id='evidence_analyst_analysis'></select><script>$('#evidence_analyst_analysis').selectpicker({liveSearch: true,title: 'Analyst',style: 'btn-outline-white'});get_request_api('/manage/users/restricted/list').done((response) => {for (index in response.data) {$('#evidence_analyst_analysis').append(`<option value='${response.data[index].user_id}'>${response.data[index].user_name}</option>`);}$('#evidence_analyst_analysis').selectpicker('refresh');});</script>\" }, \"User ID\" : { \"type\" : \"input_string\" , \"mandatory\" : false , \"value\" : \"\" }, \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } Now, the idea is to detect a change in the dropdown, put the value in the new input_string field to save our data. And then upon load, check the value in the input field and select the corresponding value in the dropdown. But how can we know the ID of the new input field we created ? The generator actually uses a convention, and the field will always have the same name if it stays at the same position in the custom attribute. If you save the current attribute and check the ID of the field with the browser debugger, you will see inpstd_2_user_id (i.e standard input in position 2 named user id ). So we can now use this ID and build up our JS. HTML template value field Corresponding custom attribute definition 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 < label > Analyzed by </ label > < select class = 'selectpicker form-control' id = 'select_evidence_analysis_analyst' ></ select > < script > $ ( '#inpstd_2_user_id' ). attr ( 'disabled' , 'disabled' ); // (1) cur_user = $ ( '#inpstd_2_user_id' ). val (); // (2) $ ( '#select_evidence_analysis_analyst' ). selectpicker ({ liveSearch : true , title : 'Analyst' , style : 'btn-outline-white' }); get_request_api ( '/manage/users/restricted/list' ) . done (( response ) => { for ( index in response . data ) { $ ( '#select_evidence_analysis_analyst' ). append ( `<option value=' ${ response . data [ index ]. user_id } '> ${ response . data [ index ]. user_name } </option>` ); } if ( cur_user !== undefined ) { $ ( '#select_evidence_analysis_analyst' ). val ( cur_user ); // (3) } $ ( '#select_evidence_analysis_analyst' ). selectpicker ( 'refresh' ); // (4) }); $ ( '#select_evidence_analysis_analyst' ). on ( 'changed.bs.select' , function ( e , clickedIndex , newValue , oldValue ) { $ ( '#inpstd_2_user_id' ). val ( $ ( e . currentTarget ). val ()); // (5) }); </ script > Disable the new input field so the users don't mess with it unintentionally Get the current value of the input in case the analyst is already set If the analyst is already set, then set the analyst ID in our Select as default Refresh the Select so the new values are taken into account Whenever the Select is changed, get the selected value and put it into the input field Final Custom attribute definition 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"Analysis\" : { \"Analyzed by JS\" : { \"type\" : \"html\" , \"mandatory\" : false , \"value\" : \"<label>Analyzed by</label><select class='selectpicker form-control' id='select_evidence_analysis_analyst'></select><script> $('#inpstd_2_user_id').attr('disabled', 'disabled'); cur_user = $('#inpstd_2_user_id').val(); $('#select_evidence_analysis_analyst').selectpicker({ liveSearch: true, title: 'Analyst', style: 'btn-outline-white' }); get_request_api('/manage/users/restricted/list') .done((response) => { for (index in response.data) { $('#select_evidence_analysis_analyst').append(`<option value='${response.data[index].user_id}'>${response.data[index].user_name}</option>`); } if (cur_user !== undefined) { $('#select_evidence_analysis_analyst').val(cur_user); } $('#select_evidence_analysis_analyst').selectpicker('refresh'); }); $('#select_evidence_analysis_analyst').on('changed.bs.select', function (e, clickedIndex, newValue, oldValue) { $('#inpstd_2_user_id').val($(e.currentTarget).val()); }); </script>\" }, \"User ID\" : { \"type\" : \"input_string\" , \"mandatory\" : false , \"value\" : \"\" }, \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } We can now remove the new lines of this snippet and place it in the value field of our HTML attribute. This is our final custom attribute ! Final Custom attribute definition 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"Analysis\" : { \"Analyzed by JS\" : { \"type\" : \"html\" , \"mandatory\" : false , \"value\" : \"<label>Analyzed by</label><select class='selectpicker form-control' id='select_evidence_analysis_analyst'></select><script> $('#inpstd_2_user_id').attr('disabled', 'disabled'); cur_user = $('#inpstd_2_user_id').val(); $('#select_evidence_analysis_analyst').selectpicker({ liveSearch: true, title: 'Analyst', style: 'btn-outline-white' }); get_request_api('/manage/users/restricted/list') .done((response) => { for (index in response.data) { $('#select_evidence_analysis_analyst').append(`<option value='${response.data[index].user_id}'>${response.data[index].user_name}</option>`); } if (cur_user !== undefined) { $('#select_evidence_analysis_analyst').val(cur_user); } $('#select_evidence_analysis_analyst').selectpicker('refresh'); }); $('#select_evidence_analysis_analyst').on('changed.bs.select', function (e, clickedIndex, newValue, oldValue) { $('#inpstd_2_user_id').val($(e.currentTarget).val()); }); </script>\" }, \"User ID\" : { \"type\" : \"input_string\" , \"mandatory\" : false , \"value\" : \"\" }, \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } We save and deploy the new custom attribute, and there we are - we can now select a user and save it ! A final note We have to admit, this was far from trivial . We are currently thinking of new ways to improve these types of specific custom attributes, and they will probably get better over time. In the meantime, we have to trick a little ! Under the hood So how does custom attributes work under the hood ? It's actually simpler than writing one as we did above. Each case objects table in the DB holds a custom_attributes field, which is of type JSON . For instance, below is the DB declaration of the Notes. When a custom attribute is created, let's say for Notes, IRIS loops over all existing Notes and apply the JSON structure you just wrote. All of the Notes are now holding this custom attribute. Rendering When the details for a Note object is requested by a user, either for creation or update, IRIS reads this custom attribute field and then starts to build a visual representation from the JSON. It does so by calling a Jinja templated HTML piece and then add this piece to the rest of the standard Note object. This Jinja template is in source/app/templates/modals/modal_attributes_tabs.html . Let's break it down and simplify it. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 <!-- Verify if the object has custom attributes --> {% if attributes and attributes|length > 0 %} <!-- Loop over each CA and build the tab --> {% for ca in attributes %} < div class = \"tab-pane\" title = \"{{ ca }}\" > <!-- Loop over each field and build them --> {% for field in attributes[ca] %} <!-- Switch case according to the field type --> {% if attributes[ca][cle][\"type\"] == \"input_string\" %} < input type = \"text\" value = \"{{ attributes[ca][cle][\" type \"][' value '] }}\" /> {% elif attributes[ca][cle][\"type\"] == \"input_checkbox\" %} < input type = \"checkbox\" value = \"<redacted for readability>\" /> {% elif attributes[ca][cle][\"type\"] == \"html\" %} {{ attributes[ca][cle][\"type\"]['value'] }} <!-- etc.. --> {% endif %} {% endfor %} </ div > {% endfor %} {% endif %} The templates loops over all attributes in the custom attribute JSON definition, and depending on the type of field, write a corresponding HTML tag. You may notice that line 18 , the HTML type directly writes the value on the template. That's what allows us to write raw HTML with Javascript ! Values saving Now, when the data is saved in the object, the client sends a JSON containing the original objet fields, as well as the custom attributes information. With the example in Basic Example , let's take a look on what happens when the data is saved. Looking at the request emitted upon Update, we can see a POST towards case/evidences/update/XX . This is the update endpoint. The payload of the request contains the standard fields of the Evidences, as well as the custom attributes. Evidence update POST payload 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"filename\" : \"My evidence\" , \"file_size\" : \"15654\" , \"file_hash\" : \"XXX\" , \"file_description\" : \"\" , \"csrf_token\" : \"REDACTED\" , \"custom_attributes\" : { \"Analysis\" : { \"Analysis note\" : \"Hello world\" , \"Has been analyzed\" : true } } } Lines 7 to 12 contains the new values of the custom attributes. When receiving the data, IRIS operates a merge. Loads the current custom attribute JSON of the Evidence Loops over the received data in custom_attributes Searches for a tab named Analysis in the loaded JSON Within it searches for a field named Analysis note , and sets its value to Hello world Within the same tab, searches for a field named Has been analyzed and sets its value to true. Saves the JSON back in the Evidence. The data is saved ! Now the custom attribute field of the Evidence looks like this : Updated custom attribute Original custom attribute 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"Analysis\" : { \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"Hello world\" } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"Analysis:\" { \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } Security note Let's consider an HTML field such as follow : 1 2 3 4 5 6 7 8 9 { \"Analysis\" : { \"Analysis header\" : { \"type\" : \"html\" , \"mandatory\" : false , \"value\" : \"<h1>Analysis</h1>\" } } } What if a user sends a request to update this field ? For instance : Malicious Evidence update POST payload 1 2 3 4 5 6 7 8 9 10 11 12 { \"filename\" : \"My evidence\" , \"file_size\" : \"15654\" , \"file_hash\" : \"XXX\" , \"file_description\" : \"\" , \"csrf_token\" : \"REDACTED\" , \"custom_attributes\" : { \"Analysis\" : { \"Analysis header\" : \"<script>Bad thing</script>\" } } } This would let the user rewrite the HTML attribute in the Evidence object. The next time another user requests this Evidence, IRIS would read the value of the attribute Analysis header , renders the malicious payload and presents it to this user. To avoid this, HTML field types are read-only for users. They can't update them. And we're done for this deep dive ! That's a long one, but hopefully this brings some lights on how the custom attributes are working behind the scene. Don't hesitate to contact us , should you have any questions or remarks. @whitekernel #Tips #Custom Attributes .md-typeset .blogging-tags-grid { display: flex; flex-direction: row; flex-wrap: wrap; gap: 8px; margin-top: 5px; } .md-typeset .blogging-tag { color: var(--md-typeset-color); background-color: var(--md-typeset-code-color); } .md-typeset .blogging-tag code { border-radius: 5px; }","title":"A deep dive into Custom Attributes"},{"location":"deep_dives/custom_attributes_dive/#a-deep-dive-into-custom-attributes","text":"In IRIS v1.4.0 we introduced the concept of Custom Attributes, a way to extend the default fields of any case objects. We already published some documentation about it, but today we are exploring the full potential of custom attributes.","title":"A deep dive into Custom Attributes"},{"location":"deep_dives/custom_attributes_dive/#a-basic-example","text":"Let's start with a simple example and extend the Evidences objects. We will : Add a new checkbox to allow analysts to indicate whether they analyzed the evidence or not, Add a text field to allow them write some notes about the analysis Heading to the attributes administration page in Advanced > Custom Attributes , we open the custom attributes for Evidences. The window should looks like this unless you already added some custom attributes. The right text input allows us to describe the custom attributes thanks to a simple JSON formatting. They are defined as follow : 1 2 3 4 5 6 7 8 9 { \"New Tab Name\" : { \"New Field Name\" : { \"type\" : \"Field type\" , \"mandatory\" : false , \"value\" : \"default value\" } } } Line 2 creates a new tab in the object Line 3 creates a new field within this new tab Line 4 to 6 describes the type and default values of this new field. Field types are predefined types, described in attributes taxonomy . So let's translate this into our example. We want to create a new tab called \"Analysis\" in Evidences object, and within these two new fields : A checkbox \"Has been analyzed\" A text field \"Analysis notes\" Checkbox is of type input_checkbox , and the text field is of type input_textfield . Putting it all together : Basic attribute 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"Analysis\" : { \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } By setting mandatory to false, we ensure the creation/update of Evidences are not rejected if these fields are not set by the users. value must be set to a boolean for checkbox inputs. It can be set to nothing for the text field. Now it's time to ensure our attributes are rendered as expected. Let's click on preview , and then on the - hopefully - new tab Analysis on the preview. There we go ! We can now deploy our changes to the Evidence objects. Curious about how it's handled under the hood ? You can take a look a the deep dive section Under the hood . We will deploy the changes to all Evidences objects but not force the overwrite if some of them already have attributes. We close the preview and we click on Update . Depending on how many Evidences you have, this can take a little time, but once the update is finished, all Evidences objects should have our new custom attributes. We head to the Evidence section of one of our case, click on one of them or try to add one, and we should have a beautiful new tab in our Evidence. Input a few notes, check the box and click on Update . The data is now saved with the object. Time to celebrate !","title":"A basic example"},{"location":"deep_dives/custom_attributes_dive/#dynamic-custom-attributes","text":"That's great, but now what if we want to add the possibility to specify who analyzed the Evidence ? We could add a new text input, but let's make something a little more advanced by proposing a list of all the analysts on the platform.","title":"Dynamic custom attributes"},{"location":"deep_dives/custom_attributes_dive/#raw-html-fields","text":"There is no direct way of proposing a list of analysts with standard custom attributes, however we have the powerful html type and thanks to that we can input HTML and so Javascript. HTML custom attributes are not processed by IRIS and are rendered as-is in the objects. Let's take the previous Evidence custom attribute and add a new html entry to it, with a simple h3 entity as HTML content. Dynamic attribute 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"Analysis\" : { \"Analyzed by JS\" : { \"type\" : \"html\" , \"mandatory\" : false , \"value\" : \"<h3>Hello IRIS</h3>\" }, \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } Which gives Nice, we can now add raw HTML to our custom attribute. IRIS uses Bootstrap to ease the UI/UX interface, so we can directly reuse these components. Let's replace the h3 with a Bootstrap Select. Note Unfortunately JSON does not support multiline strings, so to ease the reading we provide a prettified version of the HTML separately, and next to it, the corresponding final Custom Attribute definition. HTML template value field Corresponding custom attribute definition 1 2 < label > Analyzed by </ label > < select class = 'selectpicker form-control' ></ select > 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"Analysis\" : { \"Analyzed by JS\" : { \"type\" : \"html\" , \"mandatory\" : false , \"value\" : \"<label>Analyzed by</label><select class='selectpicker form-control'></select>\" }, \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } If you preview this, you'll notice that you see nothing. That's because we need to add some JS to make the select load. First we give the select an ID ( evidence_analyst_analysis ) so we can reference it in the JS, and then call the SelectPicker JS loader on it. HTML template value field Corresponding custom attribute definition 1 2 3 4 5 6 7 8 9 10 11 12 < label > Analyzed by </ label > < select class = 'selectpicker form-control' id = 'evidence_analyst_analysis' ></ select > < script > $ ( '#evidence_analyst_analysis' ). selectpicker ({ liveSearch : true , title : 'Analyst' , style : \"btn-outline-white\" }); </ script > 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"Analysis\" : { \"Analyzed by JS\" : { \"type\" : \"html\" , \"mandatory\" : false , \"value\" : \"<label>Analyzed by</label><select class='selectpicker form-control' id='evidence_analyst_analysis'></select><script>$('#evidence_analyst_analysis').selectpicker({liveSearch: true,title: 'Analyst',style: 'btn-outline-white'});</script>\" }, \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } We can now preview it. Good, we're on the right way !","title":"Raw HTML fields"},{"location":"deep_dives/custom_attributes_dive/#requesting-external-resources","text":"We now need to fill the select picker with the available analysts names. Fortunately, we have an API endpoint for that and we can add some JS to request it and fill our select . There's a few JS glue already written within IRIS, so we're going to use that to ease the writing. HTML template value field Corresponding custom attribute definition 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 < label > Analyzed by </ label > < select class = 'selectpicker form-control' id = 'evidence_analyst_analysis' ></ select > < script > $ ( '#evidence_analyst_analysis' ). selectpicker ({ liveSearch : true , title : 'Analyst' , style : 'btn-outline-white' }); get_request_api ( '/manage/users/restricted/list' ) . done (( response ) => { for ( index in response . data ) { $ ( '#evidence_analyst_analysis' ). append ( `<option value=' ${ response . data [ index ]. user_id } '> ${ response . data [ index ]. user_name } </option>` ); } $ ( '#evidence_analyst_analysis' ). selectpicker ( 'refresh' ); }); </ script > 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"Analysis\" : { \"Analyzed by JS\" : { \"type\" : \"html\" , \"mandatory\" : false , \"value\" : \"<label>Analyzed by</label><select class='selectpicker form-control' id='evidence_analyst_analysis'></select><script>$('#evidence_analyst_analysis').selectpicker({liveSearch: true,title: 'Analyst',style: 'btn-outline-white'});get_request_api('/manage/users/restricted/list').done((response) => {for (index in response.data) {$('#evidence_analyst_analysis').append(`<option value='${response.data[index].user_id}'>${response.data[index].user_name}</option>`);}$('#evidence_analyst_analysis').selectpicker('refresh');});</script>\" }, \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } Let's break it down a little. Line 2 declares the select picker for our analysts to choose a name from Lines 4 to 8 initiate the selectpicker lib on the select, so we have live search, bootstrap theme etc. Line 10 asynchronously requests the API endpoint to get a list of the users Lines 11 to 15 is a promise that is called upon request completion, add the users options to the select and finally refresh the select with the newly added options. If we now look at this new code, we should see the list of analysts. We are getting close ! Now, if you deploy these changes and try to save the information of our tab, you'll see that it doesn't work and only the Analysis note and Has been analyzed fields are saved. The Analyst information is lost. That's because HTML custom attributes cannot be saved by users, otherwise this would open the platform to a multitude of vulnerabilities. Please see section Values saving for more information. What we can do instead, is add a new input field and use it as a saving mechanism.","title":"Requesting external resources"},{"location":"deep_dives/custom_attributes_dive/#saving-values-with-html-fields","text":"So let's add an input_string field named User ID in our custom attribute, just below the HTML one. This should look like this. Addition of an input string 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"Analysis\" : { \"Analyzed by JS\" : { \"type\" : \"html\" , \"mandatory\" : false , \"value\" : \"<label>Analyzed by</label><select class='selectpicker form-control' id='evidence_analyst_analysis'></select><script>$('#evidence_analyst_analysis').selectpicker({liveSearch: true,title: 'Analyst',style: 'btn-outline-white'});get_request_api('/manage/users/restricted/list').done((response) => {for (index in response.data) {$('#evidence_analyst_analysis').append(`<option value='${response.data[index].user_id}'>${response.data[index].user_name}</option>`);}$('#evidence_analyst_analysis').selectpicker('refresh');});</script>\" }, \"User ID\" : { \"type\" : \"input_string\" , \"mandatory\" : false , \"value\" : \"\" }, \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } Now, the idea is to detect a change in the dropdown, put the value in the new input_string field to save our data. And then upon load, check the value in the input field and select the corresponding value in the dropdown. But how can we know the ID of the new input field we created ? The generator actually uses a convention, and the field will always have the same name if it stays at the same position in the custom attribute. If you save the current attribute and check the ID of the field with the browser debugger, you will see inpstd_2_user_id (i.e standard input in position 2 named user id ). So we can now use this ID and build up our JS. HTML template value field Corresponding custom attribute definition 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 < label > Analyzed by </ label > < select class = 'selectpicker form-control' id = 'select_evidence_analysis_analyst' ></ select > < script > $ ( '#inpstd_2_user_id' ). attr ( 'disabled' , 'disabled' ); // (1) cur_user = $ ( '#inpstd_2_user_id' ). val (); // (2) $ ( '#select_evidence_analysis_analyst' ). selectpicker ({ liveSearch : true , title : 'Analyst' , style : 'btn-outline-white' }); get_request_api ( '/manage/users/restricted/list' ) . done (( response ) => { for ( index in response . data ) { $ ( '#select_evidence_analysis_analyst' ). append ( `<option value=' ${ response . data [ index ]. user_id } '> ${ response . data [ index ]. user_name } </option>` ); } if ( cur_user !== undefined ) { $ ( '#select_evidence_analysis_analyst' ). val ( cur_user ); // (3) } $ ( '#select_evidence_analysis_analyst' ). selectpicker ( 'refresh' ); // (4) }); $ ( '#select_evidence_analysis_analyst' ). on ( 'changed.bs.select' , function ( e , clickedIndex , newValue , oldValue ) { $ ( '#inpstd_2_user_id' ). val ( $ ( e . currentTarget ). val ()); // (5) }); </ script > Disable the new input field so the users don't mess with it unintentionally Get the current value of the input in case the analyst is already set If the analyst is already set, then set the analyst ID in our Select as default Refresh the Select so the new values are taken into account Whenever the Select is changed, get the selected value and put it into the input field Final Custom attribute definition 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"Analysis\" : { \"Analyzed by JS\" : { \"type\" : \"html\" , \"mandatory\" : false , \"value\" : \"<label>Analyzed by</label><select class='selectpicker form-control' id='select_evidence_analysis_analyst'></select><script> $('#inpstd_2_user_id').attr('disabled', 'disabled'); cur_user = $('#inpstd_2_user_id').val(); $('#select_evidence_analysis_analyst').selectpicker({ liveSearch: true, title: 'Analyst', style: 'btn-outline-white' }); get_request_api('/manage/users/restricted/list') .done((response) => { for (index in response.data) { $('#select_evidence_analysis_analyst').append(`<option value='${response.data[index].user_id}'>${response.data[index].user_name}</option>`); } if (cur_user !== undefined) { $('#select_evidence_analysis_analyst').val(cur_user); } $('#select_evidence_analysis_analyst').selectpicker('refresh'); }); $('#select_evidence_analysis_analyst').on('changed.bs.select', function (e, clickedIndex, newValue, oldValue) { $('#inpstd_2_user_id').val($(e.currentTarget).val()); }); </script>\" }, \"User ID\" : { \"type\" : \"input_string\" , \"mandatory\" : false , \"value\" : \"\" }, \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } We can now remove the new lines of this snippet and place it in the value field of our HTML attribute. This is our final custom attribute ! Final Custom attribute definition 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"Analysis\" : { \"Analyzed by JS\" : { \"type\" : \"html\" , \"mandatory\" : false , \"value\" : \"<label>Analyzed by</label><select class='selectpicker form-control' id='select_evidence_analysis_analyst'></select><script> $('#inpstd_2_user_id').attr('disabled', 'disabled'); cur_user = $('#inpstd_2_user_id').val(); $('#select_evidence_analysis_analyst').selectpicker({ liveSearch: true, title: 'Analyst', style: 'btn-outline-white' }); get_request_api('/manage/users/restricted/list') .done((response) => { for (index in response.data) { $('#select_evidence_analysis_analyst').append(`<option value='${response.data[index].user_id}'>${response.data[index].user_name}</option>`); } if (cur_user !== undefined) { $('#select_evidence_analysis_analyst').val(cur_user); } $('#select_evidence_analysis_analyst').selectpicker('refresh'); }); $('#select_evidence_analysis_analyst').on('changed.bs.select', function (e, clickedIndex, newValue, oldValue) { $('#inpstd_2_user_id').val($(e.currentTarget).val()); }); </script>\" }, \"User ID\" : { \"type\" : \"input_string\" , \"mandatory\" : false , \"value\" : \"\" }, \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } We save and deploy the new custom attribute, and there we are - we can now select a user and save it !","title":"Saving values with HTML fields"},{"location":"deep_dives/custom_attributes_dive/#a-final-note","text":"We have to admit, this was far from trivial . We are currently thinking of new ways to improve these types of specific custom attributes, and they will probably get better over time. In the meantime, we have to trick a little !","title":"A final note"},{"location":"deep_dives/custom_attributes_dive/#under-the-hood","text":"So how does custom attributes work under the hood ? It's actually simpler than writing one as we did above. Each case objects table in the DB holds a custom_attributes field, which is of type JSON . For instance, below is the DB declaration of the Notes. When a custom attribute is created, let's say for Notes, IRIS loops over all existing Notes and apply the JSON structure you just wrote. All of the Notes are now holding this custom attribute.","title":"Under the hood"},{"location":"deep_dives/custom_attributes_dive/#rendering","text":"When the details for a Note object is requested by a user, either for creation or update, IRIS reads this custom attribute field and then starts to build a visual representation from the JSON. It does so by calling a Jinja templated HTML piece and then add this piece to the rest of the standard Note object. This Jinja template is in source/app/templates/modals/modal_attributes_tabs.html . Let's break it down and simplify it. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 <!-- Verify if the object has custom attributes --> {% if attributes and attributes|length > 0 %} <!-- Loop over each CA and build the tab --> {% for ca in attributes %} < div class = \"tab-pane\" title = \"{{ ca }}\" > <!-- Loop over each field and build them --> {% for field in attributes[ca] %} <!-- Switch case according to the field type --> {% if attributes[ca][cle][\"type\"] == \"input_string\" %} < input type = \"text\" value = \"{{ attributes[ca][cle][\" type \"][' value '] }}\" /> {% elif attributes[ca][cle][\"type\"] == \"input_checkbox\" %} < input type = \"checkbox\" value = \"<redacted for readability>\" /> {% elif attributes[ca][cle][\"type\"] == \"html\" %} {{ attributes[ca][cle][\"type\"]['value'] }} <!-- etc.. --> {% endif %} {% endfor %} </ div > {% endfor %} {% endif %} The templates loops over all attributes in the custom attribute JSON definition, and depending on the type of field, write a corresponding HTML tag. You may notice that line 18 , the HTML type directly writes the value on the template. That's what allows us to write raw HTML with Javascript !","title":"Rendering"},{"location":"deep_dives/custom_attributes_dive/#values-saving","text":"Now, when the data is saved in the object, the client sends a JSON containing the original objet fields, as well as the custom attributes information. With the example in Basic Example , let's take a look on what happens when the data is saved. Looking at the request emitted upon Update, we can see a POST towards case/evidences/update/XX . This is the update endpoint. The payload of the request contains the standard fields of the Evidences, as well as the custom attributes. Evidence update POST payload 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"filename\" : \"My evidence\" , \"file_size\" : \"15654\" , \"file_hash\" : \"XXX\" , \"file_description\" : \"\" , \"csrf_token\" : \"REDACTED\" , \"custom_attributes\" : { \"Analysis\" : { \"Analysis note\" : \"Hello world\" , \"Has been analyzed\" : true } } } Lines 7 to 12 contains the new values of the custom attributes. When receiving the data, IRIS operates a merge. Loads the current custom attribute JSON of the Evidence Loops over the received data in custom_attributes Searches for a tab named Analysis in the loaded JSON Within it searches for a field named Analysis note , and sets its value to Hello world Within the same tab, searches for a field named Has been analyzed and sets its value to true. Saves the JSON back in the Evidence. The data is saved ! Now the custom attribute field of the Evidence looks like this : Updated custom attribute Original custom attribute 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"Analysis\" : { \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"Hello world\" } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"Analysis:\" { \"Has been analyzed\" : { \"type\" : \"input_checkbox\" , \"mandatory\" : false , \"value\" : false }, \"Analysis note\" : { \"type\" : \"input_textfield\" , \"mandatory\" : false , \"value\" : \"\" } } } Security note Let's consider an HTML field such as follow : 1 2 3 4 5 6 7 8 9 { \"Analysis\" : { \"Analysis header\" : { \"type\" : \"html\" , \"mandatory\" : false , \"value\" : \"<h1>Analysis</h1>\" } } } What if a user sends a request to update this field ? For instance : Malicious Evidence update POST payload 1 2 3 4 5 6 7 8 9 10 11 12 { \"filename\" : \"My evidence\" , \"file_size\" : \"15654\" , \"file_hash\" : \"XXX\" , \"file_description\" : \"\" , \"csrf_token\" : \"REDACTED\" , \"custom_attributes\" : { \"Analysis\" : { \"Analysis header\" : \"<script>Bad thing</script>\" } } } This would let the user rewrite the HTML attribute in the Evidence object. The next time another user requests this Evidence, IRIS would read the value of the attribute Analysis header , renders the malicious payload and presents it to this user. To avoid this, HTML field types are read-only for users. They can't update them. And we're done for this deep dive ! That's a long one, but hopefully this brings some lights on how the custom attributes are working behind the scene. Don't hesitate to contact us , should you have any questions or remarks. @whitekernel #Tips #Custom Attributes .md-typeset .blogging-tags-grid { display: flex; flex-direction: row; flex-wrap: wrap; gap: 8px; margin-top: 5px; } .md-typeset .blogging-tag { color: var(--md-typeset-color); background-color: var(--md-typeset-code-color); } .md-typeset .blogging-tag code { border-radius: 5px; }","title":"Values saving"},{"location":"deep_dives/iris_module_walkthough_p1/","tags":["Walkthrough","Tips","Module"],"text":"DFIR-IRIS Modules walkthrough Part 1: A Skeleton Dance The topic of this article is to demonstrate how to get started with the development of a new module, and what structure to follow. To help this purpose, we developed a DFIR-IRIS Module generator called iris-skeleton-module that takes care of all the boilerplate code for either pipeline or processor modules. DFIR-IRIS Module (DIM) overview Before jumping into the real purpose of this article, let's have an overview on DFIR-IRIS Modules. If you already feel familiar with this topic, you can skip this part and go to the next section. IRIS can be extended with modules since its publication on GitHub end of 2021. We call them DFIR-IRIS Modules (DIMs). And no, they are not related to the underwear brand. As the documentation specifies: A DIM is a Python package allowing to extend IRIS features. DIMs are not running constantly and are only called following specific actions done by users. Pipeline module Back then, we could already use pipeline modules in order to ingest files from IRIS and process them accordingly. For example, there is a DIM called iris_evtx that ingests EVTX files from IRIS. It converts them in JSON and pushes the result into Splunk. Figure 1 - iris-evtx DIM pipeline The text input boxes in figure 1 such as index_evtx are pipeline arguments directly passed to the module. More information later in the next section. Once the EVTX are processed, the status of the task can be consulted in the \"DIM tasks\" tab: Figure 2 - iris-evtx DIM task sum up Figure 3 - iris-evtx DIM task details Processor module IRIS v1.4.0 introduced a new type of module, the processor module. Processor modules can enrich any case object: Note, Ioc, Asset, Evidence, and even a Case. The most obvious use is the enrichment of the indicators of compromise (IOCs), because during investigations analysts need to qualify any suspicious element they encounter. For example, there is a module called iris_vt_module that enriches some types of IOCs (hashes, IP addresses, domains and URLs) with VirusTotal intelligence. Figure 4 - iris-vt-module manual hook Thanks to the custom attributes (more insights about this in this previous article A deep dive into Custom Attributes ), the targeted indicator of compromise gets a new tab called \"VT Report\", displaying the results of the enrichment with iris-vt-module in a nice way. The module can also directly interact with the attributes of the IOC object: to add a new tag saying it's malicious, a description, etc. Figure 5 - VirusTotal custom attribute on IOC object Processor modules tasks status can also be consulted in the DIM tasks . Figure 6 - iris-vt-module DIM task sum up Figure 7 - iris-vt-module DIM task details The task can be triggered manually as shown in figure 4, or automatically. How does it work? IRIS implements a set of hooks triggered before or after an object (e.g. an IOC) is created or updated to the database. The iris-vt-module module uses these hooks to automatically enrich a new IOC. A hook triggered before the object is committed to the database has the on_preload_something prefix. And a hook triggered after the object is committed to the database has the on_postload_something prefix. In this scenario, the module registers functions to two different hooks: on_postload_ioc_create and on_postload_ioc_update . If we take the first hook, it means that once an IOC is created in the database, the on_postload_ioc_create hook is triggered. The module then receives both the event and the newly created IOC, executes the task accordingly (e.g: query a domain on VirusTotal and parse the report), and finally adds new information to this IOC (as in figure 5). As a final note on this overview, a module cannot be of two types at the moment. Either it's a pipeline module, or it's a processor module (it will matter for the next part). I did not write about how to import and configure a module. If you want more information about this, you can check the documentation: module management . DIMs development The documentation explains well the structure of DIMs and the interface, so I won't cover it too much. If you want to develop DIMs I heavily recommend to follow this overview and the tutorial: modules development . This section only sums up the purpose of the interface, and adds some tips about DIM development based on my experience. The interface Both types of DIMs follow the same internal structure. As said earlier, they are Python packages, and they both inherit from a common class named IrisModuleInterface . This interface is included in the Python library iris-module-interface . Thanks to this interface, the modules implement methods that IRIS recognises and uses to pass data to them and receive results back. Let's see a description of important attributes and methods implemented by the interface: _module_configuration (attribute): A list of dictionaries containing the configuration of the module. It will be shown in the UI of the module management tab. For example, with iris-vt-module you can see below a part of its module_configuration, and how it is rendered in IRIS: module_configuration = [ { \"param_name\" : \"vt_api_key\" , \"param_human_name\" : \"VT API Key\" , \"param_description\" : \"API key to use to communicate with VT\" , \"default\" : None , \"mandatory\" : True , \"type\" : \"sensitive_string\" }, { \"param_name\" : \"vt_key_is_premium\" , \"param_human_name\" : \"VT Key is premium\" , \"param_description\" : \"Set to True if the VT key is premium\" , \"default\" : False , \"mandatory\" : True , \"type\" : \"bool\" }, [ ... ] ] Code: IrisVTConfig.py#L28 Figure 8 - iris-vt-module configuration management As you can see, the configuration can be edited directly from the UI. _pipeline_info (attribute): Only useful for pipeline modules. It is used to specify arguments expected by the pipeline DIM in addition to receiving files. For example, iris-evtx module needs a Splunk index to push parsed EVTX data to Splunk. If the index does not exist the module will create it, but it still needs a name. To solve that, it asks for a required argument index_evtx . An optional argument hostname_evtx is available if a user wants to force a value in the host field (on Splunk). You can see how it is rendered in figure 1. pipeline_info = { \"pipeline_internal_name\" : \"evtx_pipeline\" , \"pipeline_human_name\" : \"EVTX Pipeline\" , \"pipeline_args\" : [ [ 'index_evtx' , 'required' ], [ 'hostname_evtx' , 'optional' ] ], \"pipeline_update_support\" : True , \"pipeline_import_support\" : True } Code: IrisEVTXModConfig.py#L26 module_dict_conf (property): use this property to get the module configuration as set by the users on the UI. The fields are specific to each module. For example, if the IRIS user passed an API key for Virustotal in the iris-vt-module DIM, it can be fetched inside the class using self.module_dict_conf.get(\"vt_api_key\") . See the definition of the configuration fields in IrisVTConfig.py . server_dict_conf (property): use this property to get settings about the IRIS server. It is particularly useful when you need to fetch the proxy parameters that have been set. To get the value: self.server_dict_conf.get('HTTPS_PROXY') or self.server_dict_conf.get('HTTP_PROXY') get_evidence_storage() : use this method to get the evidence storage. The evidence storage contains the metadata of files ingested from modules, or directly input from the web UI (the Evidences tab) or the API. For example, the pipeline module can use the evidence storage to import the metadata of files successfully processed. That's the case for iris-evtx-module . In figure 1, a few EVTX files have been successfully imported. They are now stored (their metadata actually) in the evidence storage of the case. Figure 9 - Evidences tab add_tab_attribute_field(obj, tab_name, field_name, field_type, field_value, mandatory=None, field_options=None) : This method can be used to add a new field to an IRIS object, such as an IOC. That is how iris-vt-module adds a custom attribute in figure 5. More information on vt_handler.py#L162 . add_tab_attribute is not part of the interface and must be imported from IRIS codebase using: from app.datamgmt.manage.manage_attribute_db import add_tab_attribute_field We plan to develop a \"proxy\" function in the module interface to avoid users importing code from IRIS platform. But at least with this example, you can see that it's possible! There are other few methods only meant to help IRIS (the platform) identifying the module: its type ( get_module_type ), its name ( get_module_name ), its description ( get_module_description ), etc. As I said, it is only useful for IRIS, you needn't use them in the module code. As stated earlier, IrisModuleInterface is a Python interface. It provides method signatures that will be implemented by the inherited modules. Some methods signatures are only useful for pipeline modules, and some others for processor modules. Let's split them in different sections for clarity. Pipeline module methods When \"not implemented\" is specified below, it means that the method signature exists in the interface but its code must be implemented by the module if needed. pipeline_init(app_info) (not implemented): For future uses. This function is called while IRIS initiates. It has to initiate the data needed by the pipeline such as initiating its database. pipeline_files_upload(base_path, file_handle, case_customer, case_name, is_update) (not implemented): This method is used by IRIS to notify the module that a user initiated a file upload with its pipeline. The module is responsible to saving the file. As illustrated in figures 1 and 9, the EVTX pushed by the user to the EVTX Pipeline are saved by this method. I suggest to use the base_path as the root directory destination for the input files ( file_handle ) pipeline_handler(pipeline_type, pipeline_data) (not implemented): this method is called by IRIS after calling pipeline_files_upload . It is meant to notify the module that there are files in the pipeline to process. pipeline_data is actually a dictionary containing different files related to the task: pipeline_args : a list of dictionaries containing pipeline arguments user and user_id : the user who requested the task case_name and case_id : the case linked to the task path : the base path containing the files saved in pipeline_files_upload . is_update : (deprecated) a boolean to know whether the case is being created or updated. Processor module methods register_hooks(module_id) (not implemented): This method is called by IRIS upon module registration. This method should call the method register_to_hook for each hook the module wishes to register. The module ID provided by IRIS in this method needs to be provided in every call to register_to_hook . register_to_hook(module_id, iris_hook_name, manual_hook_name, run_asynchronously) : The purpose of this method is then to register to an IRIS hook. The module ID (given by register_hooks ) and iris_hook_name must be set. If is_manual_hook is set in the module configuration, a manual_hook_name can also be set (it will be displayed in the UI) otherwise a name will be automatically built by IRIS from the module and hook name. Finally, if run_asynchronously is set (default), the action will be sent to RabbitMQ and processed asynchronously. Otherwise, the action is immediately handled, which means it needs to be quick otherwise the request will be pending and user experience degraded. Moreover, on_preload hooks (as described in the first section) are synchronous anyway because the object is not yet saved in the database. So the run_asynchronously will be ignored in that case. As an example, here is how the VT DIM registers to the on_postload_create hook: self.register_to_hook(module_id, iris_hook_name='on_postload_ioc_create') . And for a manual hook: self.register_to_hook(module_id, iris_hook_name='on_manual_trigger_ioc', manual_hook_name='Get VT insight') . deregister_from_hook(module_id, iris_hook_name) : Unregister from an existing hook. The hook_name should be a well-known hook to IRIS. No error are thrown if the hook wasn't register in the first place. For example, the iris-vt-module unregisters to the on_postload_create hook like this: self.deregister_from_hook(module_id=self.module_id, iris_hook_name='on_postload_ioc_create') . hooks_handler(hook_name, hook_ui_name, data) (not implemented): This is the actual method that receives the events triggered by the IRIS hooks the module previously registered to, thanks to register_hooks method. It receives also an argument called data . For clarity, data is a list of IRIS objects related to the event. It will allow the module to manipulate the objects and add new information. The purpose of hooks_handler is simply to dispatch the received event to a routine. For example, the VT DIM dispatches the events on_postload_ioc_create and on_postload_ioc_update to the same custom method _handle_ioc . Below you can see a snippet of code borrowed from _handle_ioc . It shows how the data argument is handled. The behaviour of this function depends on the type of IOC (domain, ip, hash...) it received. for element in data : # Check that the IOC we receive is of type the module can handle and dispatch if 'ip-' in element . ioc_type . type_name : status = vt_handler . handle_vt_ip ( ioc = element ) in_status = InterfaceStatus . merge_status ( in_status , status ) elif 'domain' in element . ioc_type . type_name : status = vt_handler . handle_vt_domain ( ioc = element ) in_status = InterfaceStatus . merge_status ( in_status , status ) elif element . ioc_type . type_name in [ 'md5' , 'sha224' , 'sha256' , 'sha512' ]: status = vt_handler . handle_vt_hash ( ioc = element ) in_status = InterfaceStatus . merge_status ( in_status , status ) else : self . log . error ( f 'IOC type { element . ioc_type . type_name } not handled by VT module. Skipping' ) Code: IrisVTInterface#L127 To conclude this section, we have seen what parts the module interface provides. Processor and pipeline module share the same codebase, but they use different methods. One must handle files (pipeline), and the other must handle IRIS objects from events based on hooks. Iris-skeleton-module: The DIM generator Despite the module interface, there is still a lot of boilerplate code to take into account when developing a new module. Boilerplate code are sections of code that are repeated in multiple modules - in this case - with little to no variation. For example iris-vt-module and iris-misp-module , both processor DIMs, share approximately 50% of the same code. In consequence, we developed (and published!) a DIM generator : iris-skeleton-module . Iris-Skeleton is a module generator based on cookiecutter and inspired by python-package-template . It is very simple to use, and it generates a new repository based on a template. To get started, cookiecutter package must be installed: pip install -U cookiecutter . Then go to a directory where you want to create your DIM and run: cookiecutter gh:dfir-iris/iris-skeleton-module In order to generate the project, cookiecutter will ask for various input variables . To sum up, it asks for a new module name, a keyword (e.g. for iris-vt-module , the keyword is VT, for iris-evtx , the keyword is EVTX), and a module description. It also asks for general information (email, license...) in order to generate the LICENSE.txt file, code headers, and the setup.py file. A Makefile is also generated. Finally, cookiecutter asks whether a pipeline or a processor module must be generated (remember that we can't have both at the same time ). Once generated, the project will contain various files: the LICENSE.txt file the README file (very basic) the setup.py file filled with provided information the Makefile, to build, install, or clean the project with ease and last but not least, the module code itself. All the boilerplate code for the chosen type of module is already there, and even some pieces of example code and module configuration. Let's create together the iris-toto-module pipeline DIM, in order to show you the process. $ pip install -U cookiecutter # Install cookiecutter into your Py env # Here you can customise your new module. Default values are between []. module_name [ iris-toto-module ] : iris-toto-module keyword [ toto ] : TOTO module_description [ ` iris-toto-module ` is a IRIS pipeline/processor module created with https://github.com/dfir-iris/iris-skeleton-module ] : organization [ iris-toto-module ] : TotoCorp Select license: 1 - MIT 2 - BSD-3 3 - GNU GPL v3.0 4 - Lesser GNU GPL v3.0 5 - Apache Software License 3 .0 Choose from 1 , 2 , 3 , 4 , 5 [ 1 ] : 4 github_name [ totocorp ] : email [ hello@totocorp.com ] : demo@totocorp.org version [ 0 .1.0 ] : Select support: 1 - pipeline 2 - processor Choose from 1 , 2 [ 1 ] : Your module iris-toto-module is created. 1 ) Now you can start working on it: $ cd iris-toto-module && git init 2 ) If you dont have pip install it 3 ) Run the make command to use pip to build the wheel package or install in your environment $ make wheel $ make install 4 ) Upload initial code to GitHub: $ git add . $ git commit -m \":tada: Initial commit\" $ git branch -M main $ git remote add origin https://github.com/totocorp/iris-toto-module.git $ git push -u origin main 5 ) grep -r \"TODO\" # to check what parts you should code For more information on how to develop IRIS modules, please consider reading the documentation: https://docs.dfir-iris.org/development/modules/. Good luck! $ cd iris-toto-module iris-toto-module $ ls AUTHORS.txt iris_toto_module LICENSE Makefile README.md setup.py iris-toto-module $ cd iris_toto_module iris-toto-module/iris_toto_module $ ls __init__.py IrisTotoConfig.py IrisTotoInterface.py toto_handler iris-toto-module/iris_toto_module $ cd toto_handler iris-toto-module/iris_toto_module/toto_handler $ ls __init__.py toto_handler.py iris-toto-module/iris_toto_module/toto_handler $ cd ../.. iris-toto-module $ make wheel pip wheel . Processing iris-toto-module Preparing metadata ( setup.py ) ... done Building wheels for collected packages: iris-toto-module Building wheel for iris-toto-module ( setup.py ) ... done Created wheel for iris-toto-module: filename = iris_toto_module-0.1.0-py3-none-any.whl size = 9302 sha256 = 5af16b1bad27b9a8f0c82c223e0939c2c57de44b24e367a65f3d38babbc1b90b Stored in directory: /home/toto/.cache/pip/wheels/7b/40/f9/adce2221081fd31117daadee7d34715b7ff076412f592e45a0 Successfully built iris-toto-module And that's all for the module generation! Now you can follow the instructions and consult the different source code files to get familiar with it. You can also generate a processor module in parallel to see the difference. Thanks to iris-skeleton-module , it is now very accessible to develop new modules. The generated modules can even be installed as is. Of course, they don't do anything special, they just print some string in the task status when used. But it shows that they already have all the necessary code to handle inputs from IRIS, and send back the results. Feel free to use it and tailor the template to your needs (and send PR ). We plan to add more input variables to cookiecutter in order to generate even more repetitive code, such as module configuration or pipeline argument dictionaries. If you develop new modules and want to share it with the community, we would be glad to add it to our organization or put a reference to your repository! The second part of this walkthrough will show the complete development of a pipeline - or maybe processor too - module using iris-skeleton-module . At the end, it will be published in the dfir-iris organization . If you want to get updates from our blog, the best way is to follow us on Twitter or join or Discord . If you have comments or questions you can reach us on Twitter, Discord, Matrix.org, and at the email address contact@dfir-iris.org. #Walkthrough #Tips #Module .md-typeset .blogging-tags-grid { display: flex; flex-direction: row; flex-wrap: wrap; gap: 8px; margin-top: 5px; } .md-typeset .blogging-tag { color: var(--md-typeset-color); background-color: var(--md-typeset-code-color); } .md-typeset .blogging-tag code { border-radius: 5px; }","title":"DFIR-IRIS Modules walkthrough Part 1: A Skeleton Dance"},{"location":"deep_dives/iris_module_walkthough_p1/#dfir-iris-modules-walkthrough-part-1-a-skeleton-dance","text":"The topic of this article is to demonstrate how to get started with the development of a new module, and what structure to follow. To help this purpose, we developed a DFIR-IRIS Module generator called iris-skeleton-module that takes care of all the boilerplate code for either pipeline or processor modules.","title":"DFIR-IRIS Modules walkthrough Part 1: A Skeleton Dance"},{"location":"deep_dives/iris_module_walkthough_p1/#dfir-iris-module-dim-overview","text":"Before jumping into the real purpose of this article, let's have an overview on DFIR-IRIS Modules. If you already feel familiar with this topic, you can skip this part and go to the next section. IRIS can be extended with modules since its publication on GitHub end of 2021. We call them DFIR-IRIS Modules (DIMs). And no, they are not related to the underwear brand. As the documentation specifies: A DIM is a Python package allowing to extend IRIS features. DIMs are not running constantly and are only called following specific actions done by users.","title":"DFIR-IRIS Module (DIM) overview"},{"location":"deep_dives/iris_module_walkthough_p1/#pipeline-module","text":"Back then, we could already use pipeline modules in order to ingest files from IRIS and process them accordingly. For example, there is a DIM called iris_evtx that ingests EVTX files from IRIS. It converts them in JSON and pushes the result into Splunk. Figure 1 - iris-evtx DIM pipeline The text input boxes in figure 1 such as index_evtx are pipeline arguments directly passed to the module. More information later in the next section. Once the EVTX are processed, the status of the task can be consulted in the \"DIM tasks\" tab: Figure 2 - iris-evtx DIM task sum up Figure 3 - iris-evtx DIM task details","title":"Pipeline module"},{"location":"deep_dives/iris_module_walkthough_p1/#processor-module","text":"IRIS v1.4.0 introduced a new type of module, the processor module. Processor modules can enrich any case object: Note, Ioc, Asset, Evidence, and even a Case. The most obvious use is the enrichment of the indicators of compromise (IOCs), because during investigations analysts need to qualify any suspicious element they encounter. For example, there is a module called iris_vt_module that enriches some types of IOCs (hashes, IP addresses, domains and URLs) with VirusTotal intelligence. Figure 4 - iris-vt-module manual hook Thanks to the custom attributes (more insights about this in this previous article A deep dive into Custom Attributes ), the targeted indicator of compromise gets a new tab called \"VT Report\", displaying the results of the enrichment with iris-vt-module in a nice way. The module can also directly interact with the attributes of the IOC object: to add a new tag saying it's malicious, a description, etc. Figure 5 - VirusTotal custom attribute on IOC object Processor modules tasks status can also be consulted in the DIM tasks . Figure 6 - iris-vt-module DIM task sum up Figure 7 - iris-vt-module DIM task details The task can be triggered manually as shown in figure 4, or automatically. How does it work? IRIS implements a set of hooks triggered before or after an object (e.g. an IOC) is created or updated to the database. The iris-vt-module module uses these hooks to automatically enrich a new IOC. A hook triggered before the object is committed to the database has the on_preload_something prefix. And a hook triggered after the object is committed to the database has the on_postload_something prefix. In this scenario, the module registers functions to two different hooks: on_postload_ioc_create and on_postload_ioc_update . If we take the first hook, it means that once an IOC is created in the database, the on_postload_ioc_create hook is triggered. The module then receives both the event and the newly created IOC, executes the task accordingly (e.g: query a domain on VirusTotal and parse the report), and finally adds new information to this IOC (as in figure 5). As a final note on this overview, a module cannot be of two types at the moment. Either it's a pipeline module, or it's a processor module (it will matter for the next part). I did not write about how to import and configure a module. If you want more information about this, you can check the documentation: module management .","title":"Processor module"},{"location":"deep_dives/iris_module_walkthough_p1/#dims-development","text":"The documentation explains well the structure of DIMs and the interface, so I won't cover it too much. If you want to develop DIMs I heavily recommend to follow this overview and the tutorial: modules development . This section only sums up the purpose of the interface, and adds some tips about DIM development based on my experience.","title":"DIMs development"},{"location":"deep_dives/iris_module_walkthough_p1/#the-interface","text":"Both types of DIMs follow the same internal structure. As said earlier, they are Python packages, and they both inherit from a common class named IrisModuleInterface . This interface is included in the Python library iris-module-interface . Thanks to this interface, the modules implement methods that IRIS recognises and uses to pass data to them and receive results back. Let's see a description of important attributes and methods implemented by the interface: _module_configuration (attribute): A list of dictionaries containing the configuration of the module. It will be shown in the UI of the module management tab. For example, with iris-vt-module you can see below a part of its module_configuration, and how it is rendered in IRIS: module_configuration = [ { \"param_name\" : \"vt_api_key\" , \"param_human_name\" : \"VT API Key\" , \"param_description\" : \"API key to use to communicate with VT\" , \"default\" : None , \"mandatory\" : True , \"type\" : \"sensitive_string\" }, { \"param_name\" : \"vt_key_is_premium\" , \"param_human_name\" : \"VT Key is premium\" , \"param_description\" : \"Set to True if the VT key is premium\" , \"default\" : False , \"mandatory\" : True , \"type\" : \"bool\" }, [ ... ] ] Code: IrisVTConfig.py#L28 Figure 8 - iris-vt-module configuration management As you can see, the configuration can be edited directly from the UI. _pipeline_info (attribute): Only useful for pipeline modules. It is used to specify arguments expected by the pipeline DIM in addition to receiving files. For example, iris-evtx module needs a Splunk index to push parsed EVTX data to Splunk. If the index does not exist the module will create it, but it still needs a name. To solve that, it asks for a required argument index_evtx . An optional argument hostname_evtx is available if a user wants to force a value in the host field (on Splunk). You can see how it is rendered in figure 1. pipeline_info = { \"pipeline_internal_name\" : \"evtx_pipeline\" , \"pipeline_human_name\" : \"EVTX Pipeline\" , \"pipeline_args\" : [ [ 'index_evtx' , 'required' ], [ 'hostname_evtx' , 'optional' ] ], \"pipeline_update_support\" : True , \"pipeline_import_support\" : True } Code: IrisEVTXModConfig.py#L26 module_dict_conf (property): use this property to get the module configuration as set by the users on the UI. The fields are specific to each module. For example, if the IRIS user passed an API key for Virustotal in the iris-vt-module DIM, it can be fetched inside the class using self.module_dict_conf.get(\"vt_api_key\") . See the definition of the configuration fields in IrisVTConfig.py . server_dict_conf (property): use this property to get settings about the IRIS server. It is particularly useful when you need to fetch the proxy parameters that have been set. To get the value: self.server_dict_conf.get('HTTPS_PROXY') or self.server_dict_conf.get('HTTP_PROXY') get_evidence_storage() : use this method to get the evidence storage. The evidence storage contains the metadata of files ingested from modules, or directly input from the web UI (the Evidences tab) or the API. For example, the pipeline module can use the evidence storage to import the metadata of files successfully processed. That's the case for iris-evtx-module . In figure 1, a few EVTX files have been successfully imported. They are now stored (their metadata actually) in the evidence storage of the case. Figure 9 - Evidences tab add_tab_attribute_field(obj, tab_name, field_name, field_type, field_value, mandatory=None, field_options=None) : This method can be used to add a new field to an IRIS object, such as an IOC. That is how iris-vt-module adds a custom attribute in figure 5. More information on vt_handler.py#L162 . add_tab_attribute is not part of the interface and must be imported from IRIS codebase using: from app.datamgmt.manage.manage_attribute_db import add_tab_attribute_field We plan to develop a \"proxy\" function in the module interface to avoid users importing code from IRIS platform. But at least with this example, you can see that it's possible! There are other few methods only meant to help IRIS (the platform) identifying the module: its type ( get_module_type ), its name ( get_module_name ), its description ( get_module_description ), etc. As I said, it is only useful for IRIS, you needn't use them in the module code. As stated earlier, IrisModuleInterface is a Python interface. It provides method signatures that will be implemented by the inherited modules. Some methods signatures are only useful for pipeline modules, and some others for processor modules. Let's split them in different sections for clarity.","title":"The interface"},{"location":"deep_dives/iris_module_walkthough_p1/#pipeline-module-methods","text":"When \"not implemented\" is specified below, it means that the method signature exists in the interface but its code must be implemented by the module if needed. pipeline_init(app_info) (not implemented): For future uses. This function is called while IRIS initiates. It has to initiate the data needed by the pipeline such as initiating its database. pipeline_files_upload(base_path, file_handle, case_customer, case_name, is_update) (not implemented): This method is used by IRIS to notify the module that a user initiated a file upload with its pipeline. The module is responsible to saving the file. As illustrated in figures 1 and 9, the EVTX pushed by the user to the EVTX Pipeline are saved by this method. I suggest to use the base_path as the root directory destination for the input files ( file_handle ) pipeline_handler(pipeline_type, pipeline_data) (not implemented): this method is called by IRIS after calling pipeline_files_upload . It is meant to notify the module that there are files in the pipeline to process. pipeline_data is actually a dictionary containing different files related to the task: pipeline_args : a list of dictionaries containing pipeline arguments user and user_id : the user who requested the task case_name and case_id : the case linked to the task path : the base path containing the files saved in pipeline_files_upload . is_update : (deprecated) a boolean to know whether the case is being created or updated.","title":"Pipeline module methods"},{"location":"deep_dives/iris_module_walkthough_p1/#processor-module-methods","text":"register_hooks(module_id) (not implemented): This method is called by IRIS upon module registration. This method should call the method register_to_hook for each hook the module wishes to register. The module ID provided by IRIS in this method needs to be provided in every call to register_to_hook . register_to_hook(module_id, iris_hook_name, manual_hook_name, run_asynchronously) : The purpose of this method is then to register to an IRIS hook. The module ID (given by register_hooks ) and iris_hook_name must be set. If is_manual_hook is set in the module configuration, a manual_hook_name can also be set (it will be displayed in the UI) otherwise a name will be automatically built by IRIS from the module and hook name. Finally, if run_asynchronously is set (default), the action will be sent to RabbitMQ and processed asynchronously. Otherwise, the action is immediately handled, which means it needs to be quick otherwise the request will be pending and user experience degraded. Moreover, on_preload hooks (as described in the first section) are synchronous anyway because the object is not yet saved in the database. So the run_asynchronously will be ignored in that case. As an example, here is how the VT DIM registers to the on_postload_create hook: self.register_to_hook(module_id, iris_hook_name='on_postload_ioc_create') . And for a manual hook: self.register_to_hook(module_id, iris_hook_name='on_manual_trigger_ioc', manual_hook_name='Get VT insight') . deregister_from_hook(module_id, iris_hook_name) : Unregister from an existing hook. The hook_name should be a well-known hook to IRIS. No error are thrown if the hook wasn't register in the first place. For example, the iris-vt-module unregisters to the on_postload_create hook like this: self.deregister_from_hook(module_id=self.module_id, iris_hook_name='on_postload_ioc_create') . hooks_handler(hook_name, hook_ui_name, data) (not implemented): This is the actual method that receives the events triggered by the IRIS hooks the module previously registered to, thanks to register_hooks method. It receives also an argument called data . For clarity, data is a list of IRIS objects related to the event. It will allow the module to manipulate the objects and add new information. The purpose of hooks_handler is simply to dispatch the received event to a routine. For example, the VT DIM dispatches the events on_postload_ioc_create and on_postload_ioc_update to the same custom method _handle_ioc . Below you can see a snippet of code borrowed from _handle_ioc . It shows how the data argument is handled. The behaviour of this function depends on the type of IOC (domain, ip, hash...) it received. for element in data : # Check that the IOC we receive is of type the module can handle and dispatch if 'ip-' in element . ioc_type . type_name : status = vt_handler . handle_vt_ip ( ioc = element ) in_status = InterfaceStatus . merge_status ( in_status , status ) elif 'domain' in element . ioc_type . type_name : status = vt_handler . handle_vt_domain ( ioc = element ) in_status = InterfaceStatus . merge_status ( in_status , status ) elif element . ioc_type . type_name in [ 'md5' , 'sha224' , 'sha256' , 'sha512' ]: status = vt_handler . handle_vt_hash ( ioc = element ) in_status = InterfaceStatus . merge_status ( in_status , status ) else : self . log . error ( f 'IOC type { element . ioc_type . type_name } not handled by VT module. Skipping' ) Code: IrisVTInterface#L127 To conclude this section, we have seen what parts the module interface provides. Processor and pipeline module share the same codebase, but they use different methods. One must handle files (pipeline), and the other must handle IRIS objects from events based on hooks.","title":"Processor module methods"},{"location":"deep_dives/iris_module_walkthough_p1/#iris-skeleton-module-the-dim-generator","text":"Despite the module interface, there is still a lot of boilerplate code to take into account when developing a new module. Boilerplate code are sections of code that are repeated in multiple modules - in this case - with little to no variation. For example iris-vt-module and iris-misp-module , both processor DIMs, share approximately 50% of the same code. In consequence, we developed (and published!) a DIM generator : iris-skeleton-module . Iris-Skeleton is a module generator based on cookiecutter and inspired by python-package-template . It is very simple to use, and it generates a new repository based on a template. To get started, cookiecutter package must be installed: pip install -U cookiecutter . Then go to a directory where you want to create your DIM and run: cookiecutter gh:dfir-iris/iris-skeleton-module In order to generate the project, cookiecutter will ask for various input variables . To sum up, it asks for a new module name, a keyword (e.g. for iris-vt-module , the keyword is VT, for iris-evtx , the keyword is EVTX), and a module description. It also asks for general information (email, license...) in order to generate the LICENSE.txt file, code headers, and the setup.py file. A Makefile is also generated. Finally, cookiecutter asks whether a pipeline or a processor module must be generated (remember that we can't have both at the same time ). Once generated, the project will contain various files: the LICENSE.txt file the README file (very basic) the setup.py file filled with provided information the Makefile, to build, install, or clean the project with ease and last but not least, the module code itself. All the boilerplate code for the chosen type of module is already there, and even some pieces of example code and module configuration. Let's create together the iris-toto-module pipeline DIM, in order to show you the process. $ pip install -U cookiecutter # Install cookiecutter into your Py env # Here you can customise your new module. Default values are between []. module_name [ iris-toto-module ] : iris-toto-module keyword [ toto ] : TOTO module_description [ ` iris-toto-module ` is a IRIS pipeline/processor module created with https://github.com/dfir-iris/iris-skeleton-module ] : organization [ iris-toto-module ] : TotoCorp Select license: 1 - MIT 2 - BSD-3 3 - GNU GPL v3.0 4 - Lesser GNU GPL v3.0 5 - Apache Software License 3 .0 Choose from 1 , 2 , 3 , 4 , 5 [ 1 ] : 4 github_name [ totocorp ] : email [ hello@totocorp.com ] : demo@totocorp.org version [ 0 .1.0 ] : Select support: 1 - pipeline 2 - processor Choose from 1 , 2 [ 1 ] : Your module iris-toto-module is created. 1 ) Now you can start working on it: $ cd iris-toto-module && git init 2 ) If you dont have pip install it 3 ) Run the make command to use pip to build the wheel package or install in your environment $ make wheel $ make install 4 ) Upload initial code to GitHub: $ git add . $ git commit -m \":tada: Initial commit\" $ git branch -M main $ git remote add origin https://github.com/totocorp/iris-toto-module.git $ git push -u origin main 5 ) grep -r \"TODO\" # to check what parts you should code For more information on how to develop IRIS modules, please consider reading the documentation: https://docs.dfir-iris.org/development/modules/. Good luck! $ cd iris-toto-module iris-toto-module $ ls AUTHORS.txt iris_toto_module LICENSE Makefile README.md setup.py iris-toto-module $ cd iris_toto_module iris-toto-module/iris_toto_module $ ls __init__.py IrisTotoConfig.py IrisTotoInterface.py toto_handler iris-toto-module/iris_toto_module $ cd toto_handler iris-toto-module/iris_toto_module/toto_handler $ ls __init__.py toto_handler.py iris-toto-module/iris_toto_module/toto_handler $ cd ../.. iris-toto-module $ make wheel pip wheel . Processing iris-toto-module Preparing metadata ( setup.py ) ... done Building wheels for collected packages: iris-toto-module Building wheel for iris-toto-module ( setup.py ) ... done Created wheel for iris-toto-module: filename = iris_toto_module-0.1.0-py3-none-any.whl size = 9302 sha256 = 5af16b1bad27b9a8f0c82c223e0939c2c57de44b24e367a65f3d38babbc1b90b Stored in directory: /home/toto/.cache/pip/wheels/7b/40/f9/adce2221081fd31117daadee7d34715b7ff076412f592e45a0 Successfully built iris-toto-module And that's all for the module generation! Now you can follow the instructions and consult the different source code files to get familiar with it. You can also generate a processor module in parallel to see the difference. Thanks to iris-skeleton-module , it is now very accessible to develop new modules. The generated modules can even be installed as is. Of course, they don't do anything special, they just print some string in the task status when used. But it shows that they already have all the necessary code to handle inputs from IRIS, and send back the results. Feel free to use it and tailor the template to your needs (and send PR ). We plan to add more input variables to cookiecutter in order to generate even more repetitive code, such as module configuration or pipeline argument dictionaries. If you develop new modules and want to share it with the community, we would be glad to add it to our organization or put a reference to your repository! The second part of this walkthrough will show the complete development of a pipeline - or maybe processor too - module using iris-skeleton-module . At the end, it will be published in the dfir-iris organization . If you want to get updates from our blog, the best way is to follow us on Twitter or join or Discord . If you have comments or questions you can reach us on Twitter, Discord, Matrix.org, and at the email address contact@dfir-iris.org. #Walkthrough #Tips #Module .md-typeset .blogging-tags-grid { display: flex; flex-direction: row; flex-wrap: wrap; gap: 8px; margin-top: 5px; } .md-typeset .blogging-tag { color: var(--md-typeset-color); background-color: var(--md-typeset-code-color); } .md-typeset .blogging-tag code { border-radius: 5px; }","title":"Iris-skeleton-module: The DIM generator"}]}